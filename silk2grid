import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.datasets import make_regression  # Sample dataset generator
import matplotlib.pyplot as plt

# Assuming your data is in a file named 'your_data.csv'
file_path = '/silk2yrs.csv'

# Load your data into a Pandas DataFrame
df = pd.read_csv(file_path)
print(df.columns)  # Check the column names in the DataFrame
df = df.apply(pd.to_numeric, errors='coerce')

# Replace 'None' values with actual NaN values for easier handling
df.replace('None', pd.NA, inplace=True)

# Separating features and target variable
X = df.drop('PM2.5', axis=1)  # Features
y = df['PM2.5']  # Target variable

# Creating SimpleImputer instance to impute missing values with the mean
imputer = SimpleImputer(strategy='most_frequent')

# Creating SimpleImputer instance to impute missing values with the mean for y
imputer_y = SimpleImputer(strategy='most_frequent')

# Impute missing values in the features
X_imputed = imputer.fit_transform(X)

# Reshape y to a 2D array
y = y.values.reshape(-1, 1)
y_imputed = imputer_y.fit_transform(y)

# Generating sample dataset (you can replace this with your own dataset)
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_imputed, test_size=0.4, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
# Create a RandomForestregressor
rf = RandomForestRegressor()
# Define the parameters to search through
param_grid = {
    'n_estimators': [500,700,1000],
    'max_depth': [None, 5, 10],
}
# Use GridSearchCV to find the best parameters
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)  # Fit on training data
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Initialize Random Forest model
rf_model = RandomForestRegressor(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Making predictions on the test set
y_pred = best_model.predict(X_test)

# Evaluating the model (optional)
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Evaluate the model
accuracy = rf_model.score(X_test, y_test)
print(f"Random Forest Accuracy: {accuracy}") 

# Create a DataFrame to hold predicted and actual values
results = pd.DataFrame({'Predicted_PM2.5': y_pred.flatten(), 'Actual_PM2.5': y_test.flatten()})

# Printing the first few rows of the DataFrame
print(results.head())

# Scatter plot of actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual PM2.5')
plt.ylabel('Predicted PM2.5')
plt.title('Actual vs. Predicted PM2.5 values')
plt.show()



Mean Squared Error: 31.665280283740863
R^2 Score: 0.8760307857309148
Root Mean Squared Error (RMSE): 5.627191154007553
Random Forest Accuracy: 0.8756539497690845
   Predicted_PM2.5  Actual_PM2.5
0        10.645220         14.50
1        18.044340         22.25
2        16.003212         16.00
3        34.631180         58.50
4        16.000000         16.00
